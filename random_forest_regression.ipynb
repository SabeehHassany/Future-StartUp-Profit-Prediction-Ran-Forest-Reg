{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "random_forest_regression.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3cvVFMz86CD",
        "colab_type": "text"
      },
      "source": [
        "Random Forest Regression is essentially  multiple decision trees working together (hence random FOREST). It's one of the most effective regression models and usually outperforms all others aside from SVR. This dataset is a simple data from Kaggle that predicts the profit of 50 startups based on 4 predictor variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2wvZ7SKXzVC",
        "colab_type": "text"
      },
      "source": [
        "### Importing the libraries\n",
        "These are the three go to libraries for most ML."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVmESEFZX4Ig",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgbK_F8-X7em",
        "colab_type": "text"
      },
      "source": [
        "### Importing the dataset\n",
        "Simple dataset import using Pandas dataframe and iloc to assign our independent variable(s) (everything besides the last column) and our dependent variable (the last column). The name of the dataset has to be updated and it must be in the same folder as your .py file or uploaded on Jupyter Notebooks or Google Collab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adBE4tjQX_Bh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = pd.read_csv('50_Startups.csv')\n",
        "X = dataset.iloc[:, :-1].values\n",
        "y = dataset.iloc[:, -1].values"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uNEvN-DN6pQg"
      },
      "source": [
        "### Encoding categorical data\n",
        "Index 3 had categorical data that had to be converted using OneHotEncoding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "si6wh6Si6pQh",
        "colab": {}
      },
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [3])], remainder='passthrough')\n",
        "X = ct.fit_transform(X)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WemVnqgeA70k",
        "colab_type": "text"
      },
      "source": [
        "### Splitting the dataset into the Training set and Test set\n",
        "Because of the smaller dataset and less variables for testing, I used a 80/20 split. The random state is tuned to 0 for consistency sakes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kb_v_ae-A-20",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4S2fyIBYDcu",
        "colab_type": "text"
      },
      "source": [
        "### Training the Random Forest Regression model on the whole dataset\n",
        "Here, 'n_estimators' is the value assigned to how many trees we have. 10 is usually the go to but with model tweaking and grid search and k fold, you can find the optimal amount for your specific problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8dOCoJ1YKMc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "e7b07bf7-e2d8-4e26-e66d-3d2b677878bd"
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "regressor = RandomForestRegressor(n_estimators = 10, random_state = 0)\n",
        "regressor.fit(X_train, y_train)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
              "                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
              "                      max_samples=None, min_impurity_decrease=0.0,\n",
              "                      min_impurity_split=None, min_samples_leaf=1,\n",
              "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
              "                      n_estimators=10, n_jobs=None, oob_score=False,\n",
              "                      random_state=0, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TzdEt-9eCU1r"
      },
      "source": [
        "### Predicting the Test set results\n",
        "By using the concatenate function I display the predicted values and  actual values in a side by side 2D array through '(len(y_wtv), 1))' for easy viewing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fmZcxSc3CU1v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "67ccf71f-4e19-459c-e73b-0735992b7db7"
      },
      "source": [
        "y_pred = regressor.predict(X_test)\n",
        "np.set_printoptions(precision=0, suppress=True)\n",
        "print(np.concatenate((y_pred.astype(int).reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[103669. 103282.]\n",
            " [133310. 144259.]\n",
            " [136197. 146122.]\n",
            " [ 81563.  77799.]\n",
            " [183960. 191050.]\n",
            " [114786. 105008.]\n",
            " [ 76475.  81229.]\n",
            " [100724.  97484.]\n",
            " [113105. 110352.]\n",
            " [162041. 166188.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SQTMVXo2MhkX"
      },
      "source": [
        "### Evaluating Model Performance\n",
        "We use two metrics to evaluate our model performance, r^2 being the more superior. These are both simple to understand and are covered in one of my Medium articles! This model acheives a r2 of .96 (.01 better than decision tree regression) with only 50 instances of testing data! Pretty impressive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiOu2RtBztA7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "705a205f-57af-4395-fb0b-b43926c6087c"
      },
      "source": [
        "from sklearn.metrics import r2_score, mean_squared_error as mse\n",
        "print(\"r^2: \" + str(r2_score(y_test, y_pred)))\n",
        "print(\"MSE: \" + str(mse(y_test, y_pred)))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "r^2: 0.9658739721928109\n",
            "MSE: 43643490.06520345\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}